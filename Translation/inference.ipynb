{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f405f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizers...\n",
      "Initializing model architecture with Attention...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model state from ltc_translator_attention.pt...\n",
      "\n",
      "--- English to French Translator ---\n",
      "Type an English sentence to translate, or \"quit\" to exit.\n",
      "\n",
      "English > how are you\n",
      "French   > comment vasz-vous ?\n",
      "\n",
      "Exiting translator.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "from enum import Enum\n",
    "\n",
    "# ========================================================================================\n",
    "# SECTION 1: Model & Preprocessing Code (MODIFIED)\n",
    "# The model definitions are updated to match the attention-based architecture.\n",
    "# ========================================================================================\n",
    "\n",
    "# --- LTCCell and Enums (No change) ---\n",
    "class MappingType(Enum):\n",
    "    Identity = 0; Linear = 1; Affine = 2\n",
    "class ODESolver(Enum):\n",
    "    SemiImplicit = 0; Explicit = 1; RungeKutta = 2\n",
    "\n",
    "class LTCCell(nn.Module):\n",
    "    # This class is copied verbatim from your training script. It does not need changes.\n",
    "    def __init__(self, input_size, num_units, solver=ODESolver.SemiImplicit, solver_clip=5.0):\n",
    "        super(LTCCell, self).__init__(); self._input_size = input_size; self._num_units = num_units; self._solver = solver\n",
    "        self._solver_clip = solver_clip; self._ode_solver_unfolds = 6; self._input_mapping = MappingType.Affine\n",
    "        self._erev_init_factor = 1; self._w_init_max = 1.0; self._w_init_min = 0.01; self._cm_init_min = 0.5\n",
    "        self._cm_init_max = 0.5; self._gleak_init_min = 1.0; self._gleak_init_max = 1.0; self._fix_cm = None\n",
    "        self._fix_gleak = None; self._fix_vleak = None; self._w_min_value = 1e-5; self._w_max_value = 1000.0\n",
    "        self._gleak_min_value = 1e-5; self._gleak_max_value = 1000.0; self._cm_t_min_value = 1e-6\n",
    "        self._cm_t_max_value = 1000.0; self._get_variables(); self._map_inputs()\n",
    "    @property\n",
    "    def state_size(self): return self._num_units\n",
    "    @property\n",
    "    def output_size(self): return self._num_units\n",
    "    def _map_inputs(self):\n",
    "        if self._input_mapping in [MappingType.Affine, MappingType.Linear]:\n",
    "            self.input_w = nn.Parameter(torch.Tensor(self._input_size)); nn.init.constant_(self.input_w, 1.0)\n",
    "        if self._input_mapping == MappingType.Affine:\n",
    "            self.input_b = nn.Parameter(torch.Tensor(self._input_size)); nn.init.constant_(self.input_b, 0.0)\n",
    "    def _get_variables(self):\n",
    "        self.sensory_mu = nn.Parameter(torch.Tensor(self._input_size, self._num_units)); self.sensory_sigma = nn.Parameter(torch.Tensor(self._input_size, self._num_units))\n",
    "        self.sensory_W = nn.Parameter(torch.Tensor(self._input_size, self._num_units)); sensory_erev_init = (2 * torch.randint(0, 2, size=[self._input_size, self._num_units]) - 1) * self._erev_init_factor\n",
    "        self.sensory_erev = nn.Parameter(sensory_erev_init.float()); nn.init.uniform_(self.sensory_mu, a=0.3, b=0.8)\n",
    "        nn.init.uniform_(self.sensory_sigma, a=3.0, b=8.0); nn.init.uniform_(self.sensory_W, a=self._w_init_min, b=self._w_init_max)\n",
    "        self.mu = nn.Parameter(torch.Tensor(self._num_units, self._num_units)); self.sigma = nn.Parameter(torch.Tensor(self._num_units, self._num_units))\n",
    "        self.W = nn.Parameter(torch.Tensor(self._num_units, self._num_units)); erev_init = (2 * torch.randint(0, 2, size=[self._num_units, self._num_units]) - 1) * self._erev_init_factor\n",
    "        self.erev = nn.Parameter(erev_init.float()); nn.init.uniform_(self.mu, a=0.3, b=0.8); nn.init.uniform_(self.sigma, a=3.0, b=8.0)\n",
    "        nn.init.uniform_(self.W, a=self._w_init_min, b=self._w_init_max)\n",
    "        if self._fix_vleak is None:\n",
    "            self.vleak = nn.Parameter(torch.Tensor(self._num_units)); nn.init.uniform_(self.vleak, a=-0.2, b=0.2)\n",
    "        else: self.register_buffer('vleak', torch.full([self._num_units], self._fix_vleak))\n",
    "        if self._fix_gleak is None:\n",
    "            self.gleak = nn.Parameter(torch.Tensor(self._num_units))\n",
    "            if self._gleak_init_max > self._gleak_init_min: nn.init.uniform_(self.gleak, a=self._gleak_init_min, b=self._gleak_init_max)\n",
    "            else: nn.init.constant_(self.gleak, self._gleak_init_min)\n",
    "        else: self.register_buffer('gleak', torch.full([self._num_units], self._fix_gleak))\n",
    "        if self._fix_cm is None:\n",
    "            self.cm_t = nn.Parameter(torch.Tensor(self._num_units))\n",
    "            if self._cm_init_max > self._cm_init_min: nn.init.uniform_(self.cm_t, a=self._cm_init_min, b=self._cm_init_max)\n",
    "            else: nn.init.constant_(self.cm_t, self._cm_init_min)\n",
    "        else: self.register_buffer('cm_t', torch.full([self._num_units], self._fix_cm))\n",
    "    def forward(self, inputs, state):\n",
    "        if self._input_mapping in [MappingType.Affine, MappingType.Linear]: inputs = inputs * self.input_w\n",
    "        if self._input_mapping == MappingType.Affine: inputs = inputs + self.input_b\n",
    "        if self._solver == ODESolver.Explicit: next_state = self._ode_step_explicit(inputs, state)\n",
    "        elif self._solver == ODESolver.SemiImplicit: next_state = self._ode_step_semi_implicit(inputs, state)\n",
    "        elif self._solver == ODESolver.RungeKutta: next_state = self._ode_step_runge_kutta(inputs, state)\n",
    "        else: raise ValueError(f\"Unknown ODE solver '{str(self._solver)}'\")\n",
    "        return next_state, next_state\n",
    "    def _ode_step_semi_implicit(self, inputs, state):\n",
    "        v_pre = state; sensory_w_activation = self.sensory_W * self._sigmoid(inputs, self.sensory_mu, self.sensory_sigma)\n",
    "        sensory_rev_activation = sensory_w_activation * self.sensory_erev; w_numerator_sensory = torch.sum(sensory_rev_activation, dim=1)\n",
    "        w_denominator_sensory = torch.sum(sensory_w_activation, dim=1)\n",
    "        for _ in range(self._ode_solver_unfolds):\n",
    "            w_activation = self.W * self._sigmoid(v_pre, self.mu, self.sigma); rev_activation = w_activation * self.erev\n",
    "            w_numerator = torch.sum(rev_activation, dim=1) + w_numerator_sensory\n",
    "            w_denominator = torch.sum(w_activation, dim=1) + w_denominator_sensory\n",
    "            numerator = self.cm_t * v_pre + self.gleak * self.vleak + w_numerator\n",
    "            denominator = self.cm_t + self.gleak + w_denominator; v_pre = numerator / denominator\n",
    "        return v_pre\n",
    "    def _f_prime(self, inputs, state):\n",
    "        v_pre = state; sensory_w_activation = self.sensory_W * self._sigmoid(inputs, self.sensory_mu, self.sensory_sigma)\n",
    "        w_reduced_sensory = torch.sum(sensory_w_activation, dim=1); w_activation = self.W * self._sigmoid(v_pre, self.mu, self.sigma)\n",
    "        w_reduced_synapse = torch.sum(w_activation, dim=1); sensory_in = self.sensory_erev * sensory_w_activation; synapse_in = self.erev * w_activation\n",
    "        sum_in = (torch.sum(sensory_in, dim=1) - v_pre * w_reduced_synapse + torch.sum(synapse_in, dim=1) - v_pre * w_reduced_sensory)\n",
    "        f_prime = (1 / self.cm_t) * (self.gleak * (self.vleak - v_pre) + sum_in); return f_prime\n",
    "    def _ode_step_explicit(self, inputs, state):\n",
    "        v_pre = state; h = 0.1\n",
    "        for _ in range(self._ode_solver_unfolds):\n",
    "            f_prime = self._f_prime(inputs, v_pre); v_pre = v_pre + h * f_prime\n",
    "            if self._solver_clip > 0: v_pre = torch.clamp(v_pre, -self._solver_clip, self._solver_clip)\n",
    "        return v_pre\n",
    "    def _ode_step_runge_kutta(self, inputs, state):\n",
    "        v_pre = state; h = 0.1\n",
    "        for _ in range(self._ode_solver_unfolds):\n",
    "            k1 = h * self._f_prime(inputs, v_pre); k2 = h * self._f_prime(inputs, v_pre + 0.5 * k1)\n",
    "            k3 = h * self._f_prime(inputs, v_pre + 0.5 * k2); k4 = h * self._f_prime(inputs, v_pre + k3)\n",
    "            v_pre = v_pre + (1.0 / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n",
    "            if self._solver_clip > 0: v_pre = torch.clamp(v_pre, -self._solver_clip, self._solver_clip)\n",
    "        return v_pre\n",
    "    def _sigmoid(self, v_pre, mu, sigma):\n",
    "        v_pre = v_pre.unsqueeze(-1); mues = v_pre - mu; x = sigma * mues; return torch.sigmoid(x)\n",
    "    def constrain_parameters(self):\n",
    "        self.cm_t.data.clamp_(min=self._cm_t_min_value, max=self._cm_t_max_value)\n",
    "        self.gleak.data.clamp_(min=self._gleak_min_value, max=self._gleak_max_value)\n",
    "        self.W.data.clamp_(min=self._w_min_value, max=self._w_max_value)\n",
    "        self.sensory_W.data.clamp_(min=self._w_min_value, max=self._w_max_value)\n",
    "\n",
    "# --- NEW/MODIFIED Model Architecture ---\n",
    "class EncoderLTC(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size):\n",
    "        super(EncoderLTC, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        self.ltc_cell_fw = LTCCell(embed_size, hidden_size)\n",
    "        self.ltc_cell_bw = LTCCell(embed_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length, batch_size = x.shape\n",
    "        hidden_state_fw = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        outputs_fw = []\n",
    "        embedding = self.embedding(x)\n",
    "        for t in range(seq_length):\n",
    "            hidden_state_fw, _ = self.ltc_cell_fw(embedding[t], hidden_state_fw)\n",
    "            outputs_fw.append(hidden_state_fw)\n",
    "        \n",
    "        hidden_state_bw = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        outputs_bw = []\n",
    "        for t in range(seq_length - 1, -1, -1):\n",
    "            hidden_state_bw, _ = self.ltc_cell_bw(embedding[t], hidden_state_bw)\n",
    "            outputs_bw.append(hidden_state_bw)\n",
    "        outputs_bw = outputs_bw[::-1]\n",
    "        \n",
    "        outputs_fw = torch.stack(outputs_fw)\n",
    "        outputs_bw = torch.stack(outputs_bw)\n",
    "        \n",
    "        encoder_outputs = torch.cat((outputs_fw, outputs_bw), dim=2)\n",
    "        final_hidden = torch.cat((hidden_state_fw, hidden_state_bw), dim=1)\n",
    "        decoder_initial_hidden = torch.tanh(self.fc(final_hidden))\n",
    "        return encoder_outputs, decoder_initial_hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        energy = torch.tanh(self.attn(torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class DecoderLTC(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size):\n",
    "        super(DecoderLTC, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = Attention(hidden_size, hidden_size)\n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        self.ltc_cell = LTCCell(embed_size + (hidden_size * 2), hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_state, encoder_outputs):\n",
    "        attention_weights = self.attention(hidden_state, encoder_outputs).unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        context_vector = torch.bmm(attention_weights, encoder_outputs).permute(1, 0, 2)\n",
    "        x = x.unsqueeze(0)\n",
    "        embedded = self.embedding(x)\n",
    "        ltc_input = torch.cat((embedded, context_vector), dim=2)\n",
    "        output, hidden_state = self.ltc_cell(ltc_input.squeeze(0), hidden_state)\n",
    "        predictions = self.fc(output)\n",
    "        return predictions, hidden_state\n",
    "\n",
    "class Seq2SeqLTC(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2SeqLTC, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "# --- Tokenizer and Preprocessing (No changes) ---\n",
    "class Tokenizer:\n",
    "    def __init__(self, encoding_name=\"cl100k_base\"):\n",
    "        import tiktoken\n",
    "        self.special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "        self._encoding = tiktoken.get_encoding(encoding_name)\n",
    "        self._base_vocab_size = self._encoding.n_vocab\n",
    "        self._offset = len(self.special_tokens)\n",
    "        self.pad_id = 0; self.sos_id = 1; self.eos_id = 2; self.unk_id = 3\n",
    "        self.vocab_size = self._base_vocab_size + self._offset\n",
    "    def encode(self, s, add_special_tokens=True):\n",
    "        s = \"\" if s is None else str(s)\n",
    "        base_tokens = self._encoding.encode(s)\n",
    "        token_ids = [t + self._offset for t in base_tokens]\n",
    "        if add_special_tokens:\n",
    "            return [self.sos_id] + token_ids + [self.eos_id]\n",
    "        return token_ids\n",
    "    def decode(self, ids):\n",
    "        base_ids = [i - self._offset for i in ids if i >= self._offset]\n",
    "        if not base_ids: return \"\"\n",
    "        return self._encoding.decode(base_ids)\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = str(w)\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = w.strip()\n",
    "    return w\n",
    "\n",
    "\n",
    "# ========================================================================================\n",
    "# SECTION 2: Inference Configuration (No changes)\n",
    "# ========================================================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_SAVE_PATH = \"ltc_translator_attention.pt\"\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "# ========================================================================================\n",
    "# SECTION 3: Translation Function (MODIFIED)\n",
    "# ========================================================================================\n",
    "def translate_sentence(model, sentence, tokenizer_en, tokenizer_fr, device, max_length=50):\n",
    "    model.eval()\n",
    "    processed_sentence = preprocess_sentence(sentence)\n",
    "    src_tokens = tokenizer_en.encode(processed_sentence)\n",
    "    src_tensor = torch.tensor(src_tokens).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encoder now returns all outputs and the initial decoder hidden state\n",
    "        encoder_outputs, hidden_state = model.encoder(src_tensor)\n",
    "\n",
    "    trg_indexes = [tokenizer_fr.sos_id]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        trg_tensor = torch.tensor([trg_indexes[-1]], device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Decoder now requires encoder_outputs at every step for attention\n",
    "            output, hidden_state = model.decoder(trg_tensor, hidden_state, encoder_outputs)\n",
    "        \n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == tokenizer_fr.eos_id:\n",
    "            break\n",
    "            \n",
    "    trg_tokens_decoded = tokenizer_fr.decode(trg_indexes)\n",
    "    return trg_tokens_decoded\n",
    "\n",
    "# ========================================================================================\n",
    "# SECTION 4: Main Execution Block (MODIFIED)\n",
    "# ========================================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(MODEL_SAVE_PATH):\n",
    "        print(f\"Error: Model file not found at '{MODEL_SAVE_PATH}'\")\n",
    "        print(\"Please run the training script first to generate the model file.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"Initializing tokenizers...\")\n",
    "    tokenizer_en = Tokenizer()\n",
    "    tokenizer_fr = Tokenizer()\n",
    "    INPUT_DIM = tokenizer_en.vocab_size\n",
    "    OUTPUT_DIM = tokenizer_fr.vocab_size\n",
    "\n",
    "    # --- Rebuild the CORRECT model architecture ---\n",
    "    print(\"Initializing model architecture with Attention...\")\n",
    "    encoder = EncoderLTC(INPUT_DIM, EMBED_SIZE, HIDDEN_SIZE)\n",
    "    decoder = DecoderLTC(OUTPUT_DIM, EMBED_SIZE, HIDDEN_SIZE)\n",
    "    model = Seq2SeqLTC(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "    print(f\"Loading model state from {MODEL_SAVE_PATH}...\")\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "\n",
    "    print(\"\\n--- English to French Translator ---\")\n",
    "    print('Type an English sentence to translate, or \"quit\" to exit.\\n')\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            sentence = input(\"English  > \")\n",
    "            if sentence.lower() in [\"quit\", \"exit\"]:\n",
    "                print(\"Exiting translator.\")\n",
    "                break\n",
    "            \n",
    "            translation = translate_sentence(model, sentence, tokenizer_en, tokenizer_fr, DEVICE)\n",
    "            print(f\"English > {sentence}\")\n",
    "            print(f\"French   > {translation}\\n\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting translator.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale up (8gb dataset on kaggle)\n",
    "# Evaluation pipeline (for benchmarking against LSTM, RNN)\n",
    "# Hybrid (SSM)\n",
    "\n",
    "# LFM vs quantized LLM on FLOPS and params\n",
    "# RAG, summarization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
