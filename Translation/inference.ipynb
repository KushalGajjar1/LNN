{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "from enum import Enum\n",
    "\n",
    "# ========================================================================================\n",
    "# SECTION 1: Model & Preprocessing Code (Copied from training script)\n",
    "# This section contains the necessary class definitions and functions\n",
    "# to rebuild the model architecture and process input text.\n",
    "# ========================================================================================\n",
    "\n",
    "# --- LTCCell and Model Definitions ---\n",
    "class MappingType(Enum):\n",
    "    Identity = 0; Linear = 1; Affine = 2\n",
    "class ODESolver(Enum):\n",
    "    SemiImplicit = 0; Explicit = 1; RungeKutta = 2\n",
    "\n",
    "class LTCCell(nn.Module):\n",
    "    # This class is copied verbatim from your training script.\n",
    "    def __init__(self, input_size, num_units, solver=ODESolver.SemiImplicit, solver_clip=5.0):\n",
    "        super(LTCCell, self).__init__(); self._input_size = input_size; self._num_units = num_units; self._solver = solver\n",
    "        self._solver_clip = solver_clip; self._ode_solver_unfolds = 6; self._input_mapping = MappingType.Affine\n",
    "        self._erev_init_factor = 1; self._w_init_max = 1.0; self._w_init_min = 0.01; self._cm_init_min = 0.5\n",
    "        self._cm_init_max = 0.5; self._gleak_init_min = 1.0; self._gleak_init_max = 1.0; self._fix_cm = None\n",
    "        self._fix_gleak = None; self._fix_vleak = None; self._w_min_value = 1e-5; self._w_max_value = 1000.0\n",
    "        self._gleak_min_value = 1e-5; self._gleak_max_value = 1000.0; self._cm_t_min_value = 1e-6\n",
    "        self._cm_t_max_value = 1000.0; self._get_variables(); self._map_inputs()\n",
    "    @property\n",
    "    def state_size(self): return self._num_units\n",
    "    @property\n",
    "    def output_size(self): return self._num_units\n",
    "    def _map_inputs(self):\n",
    "        if self._input_mapping in [MappingType.Affine, MappingType.Linear]:\n",
    "            self.input_w = nn.Parameter(torch.Tensor(self._input_size)); nn.init.constant_(self.input_w, 1.0)\n",
    "        if self._input_mapping == MappingType.Affine:\n",
    "            self.input_b = nn.Parameter(torch.Tensor(self._input_size)); nn.init.constant_(self.input_b, 0.0)\n",
    "    def _get_variables(self):\n",
    "        self.sensory_mu = nn.Parameter(torch.Tensor(self._input_size, self._num_units)); self.sensory_sigma = nn.Parameter(torch.Tensor(self._input_size, self._num_units))\n",
    "        self.sensory_W = nn.Parameter(torch.Tensor(self._input_size, self._num_units)); sensory_erev_init = (2 * torch.randint(0, 2, size=[self._input_size, self._num_units]) - 1) * self._erev_init_factor\n",
    "        self.sensory_erev = nn.Parameter(sensory_erev_init.float()); nn.init.uniform_(self.sensory_mu, a=0.3, b=0.8)\n",
    "        nn.init.uniform_(self.sensory_sigma, a=3.0, b=8.0); nn.init.uniform_(self.sensory_W, a=self._w_init_min, b=self._w_init_max)\n",
    "        self.mu = nn.Parameter(torch.Tensor(self._num_units, self._num_units)); self.sigma = nn.Parameter(torch.Tensor(self._num_units, self._num_units))\n",
    "        self.W = nn.Parameter(torch.Tensor(self._num_units, self._num_units)); erev_init = (2 * torch.randint(0, 2, size=[self._num_units, self._num_units]) - 1) * self._erev_init_factor\n",
    "        self.erev = nn.Parameter(erev_init.float()); nn.init.uniform_(self.mu, a=0.3, b=0.8); nn.init.uniform_(self.sigma, a=3.0, b=8.0)\n",
    "        nn.init.uniform_(self.W, a=self._w_init_min, b=self._w_init_max)\n",
    "        if self._fix_vleak is None:\n",
    "            self.vleak = nn.Parameter(torch.Tensor(self._num_units)); nn.init.uniform_(self.vleak, a=-0.2, b=0.2)\n",
    "        else: self.register_buffer('vleak', torch.full([self._num_units], self._fix_vleak))\n",
    "        if self._fix_gleak is None:\n",
    "            self.gleak = nn.Parameter(torch.Tensor(self._num_units))\n",
    "            if self._gleak_init_max > self._gleak_init_min: nn.init.uniform_(self.gleak, a=self._gleak_init_min, b=self._gleak_init_max)\n",
    "            else: nn.init.constant_(self.gleak, self._gleak_init_min)\n",
    "        else: self.register_buffer('gleak', torch.full([self._num_units], self._fix_gleak))\n",
    "        if self._fix_cm is None:\n",
    "            self.cm_t = nn.Parameter(torch.Tensor(self._num_units))\n",
    "            if self._cm_init_max > self._cm_init_min: nn.init.uniform_(self.cm_t, a=self._cm_init_min, b=self._cm_init_max)\n",
    "            else: nn.init.constant_(self.cm_t, self._cm_init_min)\n",
    "        else: self.register_buffer('cm_t', torch.full([self._num_units], self._fix_cm))\n",
    "    def forward(self, inputs, state):\n",
    "        if self._input_mapping in [MappingType.Affine, MappingType.Linear]: inputs = inputs * self.input_w\n",
    "        if self._input_mapping == MappingType.Affine: inputs = inputs + self.input_b\n",
    "        if self._solver == ODESolver.Explicit: next_state = self._ode_step_explicit(inputs, state)\n",
    "        elif self._solver == ODESolver.SemiImplicit: next_state = self._ode_step_semi_implicit(inputs, state)\n",
    "        elif self._solver == ODESolver.RungeKutta: next_state = self._ode_step_runge_kutta(inputs, state)\n",
    "        else: raise ValueError(f\"Unknown ODE solver '{str(self._solver)}'\")\n",
    "        return next_state, next_state\n",
    "    def _ode_step_semi_implicit(self, inputs, state):\n",
    "        v_pre = state; sensory_w_activation = self.sensory_W * self._sigmoid(inputs, self.sensory_mu, self.sensory_sigma)\n",
    "        sensory_rev_activation = sensory_w_activation * self.sensory_erev; w_numerator_sensory = torch.sum(sensory_rev_activation, dim=1)\n",
    "        w_denominator_sensory = torch.sum(sensory_w_activation, dim=1)\n",
    "        for _ in range(self._ode_solver_unfolds):\n",
    "            w_activation = self.W * self._sigmoid(v_pre, self.mu, self.sigma); rev_activation = w_activation * self.erev\n",
    "            w_numerator = torch.sum(rev_activation, dim=1) + w_numerator_sensory\n",
    "            w_denominator = torch.sum(w_activation, dim=1) + w_denominator_sensory\n",
    "            numerator = self.cm_t * v_pre + self.gleak * self.vleak + w_numerator\n",
    "            denominator = self.cm_t + self.gleak + w_denominator; v_pre = numerator / denominator\n",
    "        return v_pre\n",
    "    def _f_prime(self, inputs, state):\n",
    "        v_pre = state; sensory_w_activation = self.sensory_W * self._sigmoid(inputs, self.sensory_mu, self.sensory_sigma)\n",
    "        w_reduced_sensory = torch.sum(sensory_w_activation, dim=1); w_activation = self.W * self._sigmoid(v_pre, self.mu, self.sigma)\n",
    "        w_reduced_synapse = torch.sum(w_activation, dim=1); sensory_in = self.sensory_erev * sensory_w_activation; synapse_in = self.erev * w_activation\n",
    "        sum_in = (torch.sum(sensory_in, dim=1) - v_pre * w_reduced_synapse + torch.sum(synapse_in, dim=1) - v_pre * w_reduced_sensory)\n",
    "        f_prime = (1 / self.cm_t) * (self.gleak * (self.vleak - v_pre) + sum_in); return f_prime\n",
    "    def _ode_step_explicit(self, inputs, state):\n",
    "        v_pre = state; h = 0.1\n",
    "        for _ in range(self._ode_solver_unfolds):\n",
    "            f_prime = self._f_prime(inputs, v_pre); v_pre = v_pre + h * f_prime\n",
    "            if self._solver_clip > 0: v_pre = torch.clamp(v_pre, -self._solver_clip, self._solver_clip)\n",
    "        return v_pre\n",
    "    def _ode_step_runge_kutta(self, inputs, state):\n",
    "        v_pre = state; h = 0.1\n",
    "        for _ in range(self._ode_solver_unfolds):\n",
    "            k1 = h * self._f_prime(inputs, v_pre); k2 = h * self._f_prime(inputs, v_pre + 0.5 * k1)\n",
    "            k3 = h * self._f_prime(inputs, v_pre + 0.5 * k2); k4 = h * self._f_prime(inputs, v_pre + k3)\n",
    "            v_pre = v_pre + (1.0 / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n",
    "            if self._solver_clip > 0: v_pre = torch.clamp(v_pre, -self._solver_clip, self._solver_clip)\n",
    "        return v_pre\n",
    "    def _sigmoid(self, v_pre, mu, sigma):\n",
    "        v_pre = v_pre.unsqueeze(-1); mues = v_pre - mu; x = sigma * mues; return torch.sigmoid(x)\n",
    "    def constrain_parameters(self):\n",
    "        self.cm_t.data.clamp_(min=self._cm_t_min_value, max=self._cm_t_max_value)\n",
    "        self.gleak.data.clamp_(min=self._gleak_min_value, max=self._gleak_max_value)\n",
    "        self.W.data.clamp_(min=self._w_min_value, max=self._w_max_value)\n",
    "        self.sensory_W.data.clamp_(min=self._w_min_value, max=self._w_max_value)\n",
    "\n",
    "class EncoderLTC(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size):\n",
    "        super(EncoderLTC, self).__init__(); self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        self.ltc_cell = LTCCell(embed_size, hidden_size)\n",
    "    def forward(self, x):\n",
    "        seq_length, batch_size = x.shape\n",
    "        hidden_state = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        embedded = self.embedding(x)\n",
    "        for t in range(seq_length):\n",
    "            hidden_state, _ = self.ltc_cell(embedded[t], hidden_state)\n",
    "        return hidden_state\n",
    "\n",
    "class DecoderLTC(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size):\n",
    "        super(DecoderLTC, self).__init__(); self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        self.ltc_cell = LTCCell(embed_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x, hidden_state):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden_state = self.ltc_cell(embedded[0], hidden_state)\n",
    "        predictions = self.fc(output)\n",
    "        return predictions, hidden_state\n",
    "\n",
    "class Seq2SeqLTC(nn.Module):\n",
    "    # This Seq2Seq class is slightly simplified as it does not need the training-specific\n",
    "    # forward pass with teacher forcing. We only need the encoder and decoder components.\n",
    "    def __init__(self, encoder, decoder, target_vocab_size, device):\n",
    "        super(Seq2SeqLTC, self).__init__(); self.encoder = encoder; self.decoder = decoder\n",
    "        self.target_vocab_size = target_vocab_size; self.device = device\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5): # This method is not used in inference\n",
    "        raise NotImplementedError(\"Use encoder and decoder directly for inference.\")\n",
    "\n",
    "# --- Tokenizer ---\n",
    "class Tokenizer:\n",
    "    def __init__(self, encoding_name=\"cl100k_base\"):\n",
    "        import tiktoken\n",
    "        self.special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "        self._encoding = tiktoken.get_encoding(encoding_name)\n",
    "        self._base_vocab_size = self._encoding.n_vocab\n",
    "        self._offset = len(self.special_tokens)\n",
    "        self.pad_id = 0; self.sos_id = 1; self.eos_id = 2; self.unk_id = 3\n",
    "        self.vocab_size = self._base_vocab_size + self._offset\n",
    "    def encode(self, s, add_special_tokens=True):\n",
    "        s = \"\" if s is None else str(s)\n",
    "        base_tokens = self._encoding.encode(s)\n",
    "        token_ids = [t + self._offset for t in base_tokens]\n",
    "        if add_special_tokens:\n",
    "            return [self.sos_id] + token_ids + [self.eos_id]\n",
    "        return token_ids\n",
    "    def decode(self, ids):\n",
    "        base_ids = [i - self._offset for i in ids if i >= self._offset]\n",
    "        if not base_ids: return \"\"\n",
    "        return self._encoding.decode(base_ids)\n",
    "\n",
    "# --- Preprocessing Functions ---\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = str(w)\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    return w\n",
    "\n",
    "# ========================================================================================\n",
    "# SECTION 2: Inference Configuration\n",
    "# ========================================================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_SAVE_PATH = \"ltc_translator_best.pt\"\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "# ========================================================================================\n",
    "# SECTION 3: Translation Function\n",
    "# ========================================================================================\n",
    "def translate_sentence(model, sentence, tokenizer_en, tokenizer_fr, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Translates a single sentence from English to French.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess and tokenize the source sentence\n",
    "    processed_sentence = preprocess_sentence(sentence)\n",
    "    src_tokens = tokenizer_en.encode(processed_sentence)\n",
    "    \n",
    "    # Add batch dimension (batch size = 1) and move to device\n",
    "    src_tensor = torch.tensor(src_tokens).unsqueeze(1).to(device)\n",
    "\n",
    "    # Disable gradient calculation for inference\n",
    "    with torch.no_grad():\n",
    "        # Get the context vector from the encoder\n",
    "        hidden_state = model.encoder(src_tensor)\n",
    "\n",
    "    # Initialize the list of target tokens with the <sos> token\n",
    "    trg_indexes = [tokenizer_fr.sos_id]\n",
    "\n",
    "    # Generate the translation token by token\n",
    "    for _ in range(max_length):\n",
    "        # Get the last predicted token\n",
    "        trg_tensor = torch.tensor([trg_indexes[-1]], device=device)\n",
    "\n",
    "        # Feed the current token and the hidden state to the decoder\n",
    "        with torch.no_grad():\n",
    "            output, hidden_state = model.decoder(trg_tensor, hidden_state)\n",
    "        \n",
    "        # Get the token with the highest probability\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        # Stop if the <eos> token is generated\n",
    "        if pred_token == tokenizer_fr.eos_id:\n",
    "            break\n",
    "            \n",
    "    # Decode the list of token IDs back to a French sentence\n",
    "    trg_tokens_decoded = tokenizer_fr.decode(trg_indexes)\n",
    "\n",
    "    return trg_tokens_decoded\n",
    "\n",
    "\n",
    "# ========================================================================================\n",
    "# SECTION 4: Main Execution Block\n",
    "# ========================================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if the model file exists\n",
    "    if not os.path.exists(MODEL_SAVE_PATH):\n",
    "        print(f\"Error: Model file not found at '{MODEL_SAVE_PATH}'\")\n",
    "        print(\"Please run the training script first to generate the model file.\")\n",
    "        exit()\n",
    "\n",
    "    # --- Initialize tokenizers ---\n",
    "    print(\"Initializing tokenizers...\")\n",
    "    tokenizer_en = Tokenizer()\n",
    "    tokenizer_fr = Tokenizer()\n",
    "    INPUT_DIM = tokenizer_en.vocab_size\n",
    "    OUTPUT_DIM = tokenizer_fr.vocab_size\n",
    "\n",
    "    # --- Rebuild the model architecture ---\n",
    "    print(\"Initializing model architecture...\")\n",
    "    encoder = EncoderLTC(INPUT_DIM, EMBED_SIZE, HIDDEN_SIZE)\n",
    "    decoder = DecoderLTC(OUTPUT_DIM, EMBED_SIZE, HIDDEN_SIZE)\n",
    "    model = Seq2SeqLTC(encoder, decoder, OUTPUT_DIM, DEVICE).to(DEVICE)\n",
    "\n",
    "    # --- Load the saved weights ---\n",
    "    print(f\"Loading model state from {MODEL_SAVE_PATH}...\")\n",
    "    # Use map_location to ensure the model loads correctly whether on CPU or GPU\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "\n",
    "    print(\"\\n--- English to French Translator ---\")\n",
    "    print('Type an English sentence to translate, or \"quit\" to exit.\\n')\n",
    "\n",
    "    # --- Interactive loop ---\n",
    "    while True:\n",
    "        try:\n",
    "            sentence = input(\"English  > \")\n",
    "            if sentence.lower() in [\"quit\", \"exit\"]:\n",
    "                print(\"Exiting translator.\")\n",
    "                break\n",
    "            \n",
    "            translation = translate_sentence(model, sentence, tokenizer_en, tokenizer_fr, DEVICE)\n",
    "            print(f\"French   > {translation}\\n\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting translator.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f405f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
